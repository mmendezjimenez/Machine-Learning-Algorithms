{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example From:\n",
    "#https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Imports \n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy data for training\n",
    "\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the model architecture\n",
    "\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the parameters for the model\n",
    "\n",
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100        #Number of iterations the algorithm will run through\n",
    "\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "#Makes use of the GPU if you have one available\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we initialize the loss function and the optimizer\n",
    "\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(136.9714, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 0, loss 136.971435546875\n",
      "tensor(11.1781, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 1, loss 11.178140640258789\n",
      "tensor(0.9175, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 2, loss 0.9175172448158264\n",
      "tensor(0.0805, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 3, loss 0.08052688837051392\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 4, loss 0.01219265442341566\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 5, loss 0.0065561337396502495\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 6, loss 0.006034263875335455\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 7, loss 0.005930277053266764\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 8, loss 0.0058610690757632256\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 9, loss 0.005795388948172331\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 10, loss 0.005730630364269018\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 11, loss 0.0056666480377316475\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 12, loss 0.005603373050689697\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 13, loss 0.005540822166949511\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 14, loss 0.005478924605995417\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 15, loss 0.005417729262262583\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 16, loss 0.005357246845960617\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 17, loss 0.005297403782606125\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 18, loss 0.00523826340213418\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 19, loss 0.005179753992706537\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 20, loss 0.005121929105371237\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 21, loss 0.005064739380031824\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 22, loss 0.005008187144994736\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 23, loss 0.004952252376824617\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 24, loss 0.004896955098956823\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 25, loss 0.0048422617837786674\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 26, loss 0.004788190126419067\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 27, loss 0.0047347224317491055\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 28, loss 0.004681842401623726\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 29, loss 0.004629551898688078\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 30, loss 0.0045778765343129635\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 31, loss 0.004526766948401928\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 32, loss 0.004476221743971109\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 33, loss 0.004426215775310993\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 34, loss 0.004376784898340702\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 35, loss 0.004327910486608744\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 36, loss 0.004279595799744129\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 37, loss 0.004231777507811785\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 38, loss 0.004184538032859564\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 39, loss 0.00413781451061368\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 40, loss 0.004091605544090271\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 41, loss 0.0040459297597408295\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 42, loss 0.004000737797468901\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 43, loss 0.0039560613222420216\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 44, loss 0.003911878913640976\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 45, loss 0.003868208033964038\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 46, loss 0.003825007937848568\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 47, loss 0.003782294224947691\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 48, loss 0.0037400606088340282\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 49, loss 0.0036982938181608915\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 50, loss 0.003657008521258831\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 51, loss 0.0036161679308861494\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 52, loss 0.0035757964942604303\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 53, loss 0.0035358499735593796\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 54, loss 0.0034963584039360285\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 55, loss 0.0034573322627693415\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 56, loss 0.003418727545067668\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 57, loss 0.0033805477432906628\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 58, loss 0.003342778654769063\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 59, loss 0.003305460326373577\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 60, loss 0.003268561791628599\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 61, loss 0.0032320593018084764\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 62, loss 0.0031959800980985165\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 63, loss 0.003160262480378151\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 64, loss 0.0031249914318323135\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 65, loss 0.003090101992711425\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 66, loss 0.0030555829871445894\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 67, loss 0.0030214544385671616\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 68, loss 0.002987728687003255\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 69, loss 0.0029543477576225996\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 70, loss 0.002921359147876501\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 71, loss 0.002888742135837674\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 72, loss 0.0028564874082803726\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 73, loss 0.0028245917055755854\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 74, loss 0.002793041290715337\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 75, loss 0.0027618459425866604\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 76, loss 0.0027309933211654425\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 77, loss 0.002700496232137084\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 78, loss 0.002670349320396781\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 79, loss 0.0026405281387269497\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 80, loss 0.002611050847917795\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 81, loss 0.002581879496574402\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 82, loss 0.0025530457496643066\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 83, loss 0.0025245442520827055\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 84, loss 0.002496349858120084\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 85, loss 0.0024684658274054527\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 86, loss 0.0024409093894064426\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 87, loss 0.0024136584252119064\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 88, loss 0.0023866978008300066\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 89, loss 0.00236004451289773\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 90, loss 0.002333688084036112\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 91, loss 0.002307637594640255\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 92, loss 0.002281858818605542\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 93, loss 0.0022563941311091185\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 94, loss 0.0022311792708933353\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 95, loss 0.002206268720328808\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 96, loss 0.0021816331427544355\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 97, loss 0.0021572732366621494\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 98, loss 0.0021331757307052612\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 99, loss 0.002109350636601448\n"
     ]
    }
   ],
   "source": [
    "#Training of the Model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():                 #Uses GPU if available\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0854349]\n",
      " [ 3.0731316]\n",
      " [ 5.060828 ]\n",
      " [ 7.0485244]\n",
      " [ 9.036221 ]\n",
      " [11.023917 ]\n",
      " [13.011614 ]\n",
      " [14.9993105]\n",
      " [16.987007 ]\n",
      " [18.974703 ]\n",
      " [20.9624   ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing the Model and Plotting\n",
    "\n",
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
